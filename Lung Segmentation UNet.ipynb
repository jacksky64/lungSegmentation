{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["#pip install albumentations > /dev/null\n","#!pip install -U efficientnet==0.0.4\n","#!pip install -U segmentation-models\n","\n","import numpy as np\n","import pandas as pd\n","import gc\n","import tensorflow.keras as keras\n","\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-white')\n","import seaborn as sns\n","sns.set_style(\"white\")\n","\n","from sklearn.model_selection import train_test_split,StratifiedKFold\n","\n","from skimage.transform import resize\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.losses import binary_crossentropy\n","\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras import Model\n","from tensorflow.keras.callbacks import  ModelCheckpoint\n","from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout,BatchNormalization\n","from tensorflow.keras.layers import Conv2D, Concatenate, MaxPooling2D\n","from tensorflow.keras.layers import UpSampling2D, Dropout, BatchNormalization\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import ZeroPadding2D\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras.layers import multiply\n","\n","\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n","from tensorflow.keras.optimizers import SGD,Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","import glob\n","import shutil\n","import os\n","import random\n","from PIL import Image\n","import cv2\n","\n","seed = 10\n","np.random.seed(seed)\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","    \n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["image_path = os.path.join(\"e:/chest-xray-masks-and-labels/data/Lung Segmentation/CXR_png/\")\n","mask_path = os.path.join(\"e:/chest-xray-masks-and-labels/data/Lung Segmentation/\",\"masks/\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["images = os.listdir(image_path)\n","mask = os.listdir(mask_path)\n","mask = [fName.split(\".png\")[0] for fName in mask]\n","image_file_name = [fName.split(\"_mask\")[0] for fName in mask]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["check = [i for i in mask if \"mask\" in i]\n","print(\"Total mask that has modified name:\",len(check))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["test_files = set(os.listdir(image_path)) & set(os.listdir(mask_path))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["l_mask = list(test_files) + [x + '.png' for x in check]\n","l_mask.sort()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["l_img = [x.split('_mask')[0] for x in check]\n","l_img = list(test_files) + [x + '.png' for x in l_img]\n","l_img.sort()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df = pd.DataFrame()\n","df['images'] = l_img\n","df['mask'] = l_mask"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df['images'] = image_path + df['images']\n","df['mask'] = mask_path + df['mask']"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print('No. of train files:', len(df))\n",""],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_im_path = df['images'].tolist()\n","train_mask_path = df['mask'].tolist()\n","# val_im_path = val_df['images'].tolist()\n","# val_mask_path = val_df['mask'].tolist()\n","img_size = 1024"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class DataGenerator(keras.utils.Sequence):\n","    \n","    def __init__(self,train_im_path = train_im_path,train_mask_path = train_mask_path,\n","                 augmentations = None,img_size = img_size,batch_size = 64,nchannels = 3,shuffle = True):\n","        \n","        #self.train_im_paths = list(filepath)\n","        self.train_im_path = train_im_path\n","        self.train_mask_path = train_mask_path\n","        self.img_size = img_size\n","        self.batch_size = batch_size\n","        self.nchannels = nchannels\n","        self.shuffle = shuffle\n","        self.augmentations = augmentations\n","        self.on_epoch_end()\n","    \n","    def __len__(self):\n","        \n","        return int(np.ceil(len(self.train_im_path)/ self.batch_size))\n","    \n","    def __getitem__(self,index):\n","        \n","        indexes = self.indexes[index * self.batch_size : min((index + 1) * self.batch_size, len(self.train_im_path))]\n","        list_im_ids = [self.train_im_path[i] for i in indexes]\n","        list_mask_ids = [self.train_mask_path[i] for i in indexes]\n","        X,y = self.data_generation(list_im_ids,list_mask_ids)\n","        \n","        return X,np.array(y) / 255.\n","    \n","    def on_epoch_end(self):\n","        \n","        self.indexes = np.arange(len(self.train_im_path))\n","        if(self.shuffle):\n","            np.random.shuffle(self.indexes)\n","    \n","    def data_generation(self,list_im_ids,list_mask_ids):\n","        \n","        X = np.empty((len(list_im_ids),self.img_size,self.img_size,self.nchannels))\n","        y = np.empty((len(list_mask_ids),self.img_size,self.img_size,1))\n","        for i,(img_path, mask_path) in enumerate(zip(list_im_ids,list_mask_ids)):\n","            #print(mask_path)\n","            mask = np.array(Image.open(mask_path))\n","            #plt.imshow(mask)\n","            img = np.array(Image.open(img_path))\n","            #img = np.true_divide(img,255.)\n","           # plt.imshow(img,cmap = 'bone')\n","            if(len(img.shape) == 2):\n","                img = np.repeat(img[...,np.newaxis],3,2)\n","            \n","           # plt.imshow(img,cmap = 'bone')\n","            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n","            y[i,] = cv2.resize(mask,(self.img_size,self.img_size))[...,np.newaxis]\n","            y[y > 0] = 255\n","        return np.uint8(X),np.uint8(y)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["a = DataGenerator(batch_size=20,shuffle=False)\n","images,masks = a.__getitem__(0)\n","max_images = 20\n","grid_width = 5\n","grid_height = int(max_images / grid_width)\n","fig, axs = plt.subplots(grid_height, grid_width, figsize=(10, 10))\n","\n","for i,(im, mask) in enumerate(zip(images,masks)):\n","    ax = axs[int(i / grid_width), i % grid_width]\n","    ax.imshow(im.squeeze(), cmap=\"bone\")\n","    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n","    ax.axis('off')\n","plt.suptitle(\"Chest X-rays, Red: Pneumothorax.\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n","    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n","    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n","    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n","    return c, p\n","\n","def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n","    us = keras.layers.UpSampling2D((2, 2))(x)\n","    concat = keras.layers.Concatenate()([us, skip])\n","    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n","    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n","    return c\n","\n","def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n","    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n","    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n","    return c"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def UNet(image_size):\n","    f = [16, 32, 64, 128, 256]\n","    inputs = keras.layers.Input((image_size, image_size, 3))\n","    \n","    p0 = inputs\n","    c1, p1 = down_block(p0, f[0]) #128 -> 64\n","    c2, p2 = down_block(p1, f[1]) #64 -> 32\n","    c3, p3 = down_block(p2, f[2]) #32 -> 16\n","    c4, p4 = down_block(p3, f[3]) #16->8\n","    \n","    bn = bottleneck(p4, f[4])\n","    \n","    u1 = up_block(bn, c4, f[3]) #8 -> 16\n","    u2 = up_block(u1, c3, f[2]) #16 -> 32\n","    u3 = up_block(u2, c2, f[1]) #32 -> 64\n","    u4 = up_block(u3, c1, f[0]) #64 -> 128\n","    \n","    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n","    model = keras.models.Model(inputs, outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def unet(input_size=(256,256,3)):\n","    inputs = keras.layers.Input((img_size, img_size, 3))\n","    \n","    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n","    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n","    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n","    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n","    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n","\n","    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n","    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n","\n","    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n","    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n","    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n","\n","    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n","    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n","    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n","\n","    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n","    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n","    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n","\n","    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n","    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n","    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n","\n","    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n","\n","    return Model(inputs=[inputs], outputs=[conv10])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# model = unet(input_size=img_size)\n","# model.summary()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import segmentation_models as sm\n","model = sm.Unet('seresnet34', input_shape=(img_size,img_size,3), encoder_weights='imagenet',decoder_block_type='transpose')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def get_iou_vector(A,B):\n","    batch_size = A.shape[0]\n","    metric = 0.0\n","    for i in range(batch_size):\n","        t,p = A[i],B[i]\n","        intersection = np.sum(t * p)\n","        true = np.sum(t)\n","        pred = np.sum(p)\n","        \n","        if(true == 0):\n","            metric += (pred == 0)\n","            \n","        union = true + pred - intersection\n","        iou = intersection / union\n","        iou = np.floor(max(0,(iou - 0.45) * 20)) / 10\n","        metric += iou\n","    return metric / batch_size\n","def iou_metric(label,pred):\n","    return tf.py_func(get_iou_vector,[label,pred > 0.5],tf.float64)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def dice_coeff(y_true,y_pred):\n","    y_true_f = K.flatten(y_true)\n","    #y_pred_f = K.cast(K.greater(K.flatten(y_pred),0.5),'float32')\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    dice_coeff = (intersection * 2) / (K.sum(y_true_f) + K.sum(y_pred_f))\n","    return dice_coeff\n","def dice_loss(y_true,y_pred):   \n","    smooth = 1\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    dice_coeff = (intersection * 2 + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    return 1 - dice_coeff\n","def bce_dice_loss(y_true,y_pred):\n","    return binary_crossentropy(y_true,y_pred) + dice_loss(y_true,y_pred)\n","def bce_logdice_loss(y_true,y_pred):\n","    return binary_crossentropy(y_true,y_pred) - K.log(1. - dice_loss(y_true,y_pred))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# !pip install -U segmentation-models\n","# import segmentation_models as sm\n","# model = sm.Unet('resnet34', input_shape=(img_size,img_size,3), encoder_weights='imagenet',decoder_block_type='transpose')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["model.compile(loss = bce_dice_loss,optimizer = Adam(3e-4))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["img_size = 1024\n","epochs = 1\n","batch_size = 4\n","train_generator = DataGenerator(train_im_path = train_im_path, train_mask_path = train_mask_path, batch_size = batch_size)\n","#val_generator = DataGenerator(train_im_path = val_im_path, train_mask_path = val_mask_path, batch_size = batch_size)\n","callbacks = [ModelCheckpoint('best_model.h5',save_best_only = True,monitor = 'iou_metric', mode = 'max', verbose = 1)]\n","history = model.fit_generator(train_generator,epochs = epochs,callbacks = callbacks)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 8))\n","ax1.plot(history.history['loss'], '-', label = 'Loss')\n","#ax1.plot(history.history['val_loss'], '-', label = 'Validation Loss')\n","ax1.legend()\n","\n","ax2.plot(np.array(history.history['iou_metric']), '-', \n","         label = 'IOU')\n","# ax2.plot(np.array(history.history['val_iou_metric']), '-',\n","#          label = 'Validation IOU')\n","ax2.legend()\n",""],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["plt.savefig('train.png')"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3.7.6 64-bit ('tf': conda)","name":"python37664bittfconda371af89065be4cc6be4c7391e4e21e44"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.7.6-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}